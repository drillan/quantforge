"""Black-Scholesã®æ€§èƒ½æ¯”è¼ƒ."""

import platform
import time
from typing import Any

import numpy as np
import psutil
from quantforge import models

from benchmarks.baseline.python_baseline import (
    black_scholes_numpy_batch,
    black_scholes_pure_python,
    black_scholes_pure_python_batch,
    black_scholes_scipy_single,
)


class BenchmarkRunner:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã‚¯ãƒ©ã‚¹."""

    def __init__(self, warmup_runs: int = 100, measure_runs: int = 1000):
        self.warmup_runs = warmup_runs
        self.measure_runs = measure_runs

    def get_system_info(self) -> dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’å–å¾—."""
        return {
            "platform": platform.platform(),
            "processor": platform.processor(),
            "cpu_count": psutil.cpu_count(logical=False),
            "cpu_count_logical": psutil.cpu_count(logical=True),
            "memory_gb": round(psutil.virtual_memory().total / (1024**3), 1),
            "python_version": platform.python_version(),
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        }

    def benchmark_single(self) -> dict[str, Any]:
        """å˜ä¸€è¨ˆç®—ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯."""
        s, k, t, r, sigma = 100.0, 100.0, 1.0, 0.05, 0.2

        results: dict[str, Any] = {}

        # QuantForge (Rust)
        for _ in range(self.warmup_runs):
            models.call_price(s, k, t, r, sigma)

        times = []
        for _ in range(self.measure_runs):
            start = time.perf_counter()
            models.call_price(s, k, t, r, sigma)
            times.append(time.perf_counter() - start)
        qf_time = np.median(times)  # ä¸­å¤®å€¤ã‚’ä½¿ç”¨ï¼ˆå¤–ã‚Œå€¤ã®å½±éŸ¿ã‚’è»½æ¸›ï¼‰
        results["quantforge"] = qf_time

        # Pure Pythonï¼ˆå¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãªã—ï¼‰
        for _ in range(min(self.warmup_runs, 10)):  # é…ã„ã®ã§å°‘ãªã‚
            black_scholes_pure_python(s, k, t, r, sigma)

        times = []
        for _ in range(min(self.measure_runs, 100)):  # é…ã„ã®ã§å°‘ãªã‚
            start = time.perf_counter()
            black_scholes_pure_python(s, k, t, r, sigma)
            times.append(time.perf_counter() - start)
        py_time = np.median(times)
        results["pure_python"] = py_time

        # NumPy+SciPyï¼ˆä¸€èˆ¬çš„ãªå®Ÿè£…ï¼‰
        for _ in range(self.warmup_runs):
            black_scholes_scipy_single(s, k, t, r, sigma)

        times = []
        for _ in range(self.measure_runs):
            start = time.perf_counter()
            black_scholes_scipy_single(s, k, t, r, sigma)
            times.append(time.perf_counter() - start)
        numpy_scipy_time = np.median(times)
        results["numpy_scipy"] = numpy_scipy_time

        # ç›¸å¯¾æ€§èƒ½è¨ˆç®—
        results["speedup_vs_pure_python"] = py_time / qf_time
        results["speedup_vs_numpy_scipy"] = numpy_scipy_time / qf_time

        return results

    def benchmark_batch(self, size: int) -> dict[str, Any]:
        """ãƒãƒƒãƒå‡¦ç†ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯."""
        spots = np.random.uniform(50, 150, size).astype(np.float64)
        spots_list = spots.tolist()  # Pure Pythonç”¨
        k, t, r, sigma = 100.0, 1.0, 0.05, 0.2

        results: dict[str, Any] = {"size": size}

        # æ¸¬å®šå›æ•°ã‚’ã‚µã‚¤ã‚ºã«å¿œã˜ã¦èª¿æ•´ï¼ˆé©åˆ‡ãªå®Ÿè¡Œæ™‚é–“ã‚’ç¢ºä¿ï¼‰
        if size <= 100:
            n_runs = 500  # å°ã•ã„ã‚µã‚¤ã‚ºã¯å¤šã‚ã«æ¸¬å®š
        elif size <= 1000:
            n_runs = 200
        elif size <= 10000:
            n_runs = 100
        elif size <= 100000:
            n_runs = 50
        else:
            n_runs = 30  # 100ä¸‡ä»¶ã§ã‚‚30å›æ¸¬å®š

        # QuantForge
        if hasattr(models, "call_price_batch"):
            # Warmup (ååˆ†ãªã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã§å®‰å®šã—ãŸæ¸¬å®š)
            for _ in range(20):
                _ = models.call_price_batch(spots[: min(100, size)], k, t, r, sigma)

            # Measure
            times = []
            for _ in range(n_runs):
                start = time.perf_counter()
                _ = models.call_price_batch(spots, k, t, r, sigma)
                times.append(time.perf_counter() - start)
            qf_time = float(np.median(times))
        else:
            qf_time = float("inf")
        results["quantforge"] = qf_time

        # NumPy Batch
        # Warmup (ååˆ†ãªã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã§å®‰å®šã—ãŸæ¸¬å®š)
        for _ in range(20):
            _ = black_scholes_numpy_batch(spots[: min(100, size)], k, t, r, sigma)

        # Measure
        times = []
        for _ in range(n_runs):
            start = time.perf_counter()
            _ = black_scholes_numpy_batch(spots, k, t, r, sigma)
            times.append(time.perf_counter() - start)
        np_time = np.median(times)
        results["numpy_batch"] = np_time

        # Pure Python (å°ã•ã„ã‚µã‚¤ã‚ºã®ã¿)
        if size <= 1000:
            # Warmup
            for _ in range(3):  # Pure Pythonã¯é…ã„ã®ã§å°‘ãªã‚
                _ = black_scholes_pure_python_batch(spots_list[: min(10, size)], k, t, r, sigma)

            # Measure
            times = []
            py_runs = min(n_runs // 10, 20) if size <= 100 else min(n_runs // 20, 10)
            for _ in range(py_runs):  # Pure Pythonã¯é…ã„ã®ã§å›æ•°åˆ¶é™
                start = time.perf_counter()
                _ = black_scholes_pure_python_batch(spots_list, k, t, r, sigma)
                times.append(time.perf_counter() - start)
            py_time = np.median(times)
            results["pure_python"] = py_time
            results["speedup_vs_pure_python"] = py_time / qf_time

        # ç›¸å¯¾æ€§èƒ½ã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ
        results["speedup_vs_numpy"] = np_time / qf_time
        results["throughput_qf"] = size / qf_time
        results["throughput_np"] = size / np_time

        return results

    def run_all(self) -> dict[str, Any]:
        """å…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ."""
        print("ğŸš€ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹...")

        results: dict[str, Any] = {"system_info": self.get_system_info(), "single": {}, "batch": []}

        print("ğŸ“Š å˜ä¸€è¨ˆç®—ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­...")
        results["single"] = self.benchmark_single()

        print("ğŸ“Š ãƒãƒƒãƒå‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­...")
        for size in [100, 1000, 10000, 100000, 1000000]:
            print(f"  - ã‚µã‚¤ã‚º {size:,} ...")
            results["batch"].append(self.benchmark_batch(size))

        # çµæœã‚’æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜
        from benchmarks.analysis.save import save_benchmark_result

        save_benchmark_result(results)

        print("âœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†")
        return results


def main() -> None:
    """Main entry point for comparison benchmarks."""
    runner = BenchmarkRunner()
    results = runner.run_all()

    # ç°¡æ˜“çµæœè¡¨ç¤º
    print("\n=== çµæœã‚µãƒãƒª ===")
    single = results["single"]
    print(f"å˜ä¸€è¨ˆç®—: QuantForgeã¯Pure Pythonã‚ˆã‚Š{single['speedup_vs_pure_python']:.0f}å€é«˜é€Ÿ")

    batch_1m = next((b for b in results["batch"] if b["size"] == 1000000), None)
    if batch_1m:
        print(f"100ä¸‡ä»¶ãƒãƒƒãƒ: QuantForgeã¯NumPyã‚ˆã‚Š{batch_1m['speedup_vs_numpy']:.1f}å€é«˜é€Ÿ")
        print(f"ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {batch_1m['throughput_qf'] / 1e6:.1f}M ops/sec")


if __name__ == "__main__":
    main()
