#!/usr/bin/env python3
"""ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ›´æ–°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ.

æœ€æ–°ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚
GitHub Actionsã®mainãƒ–ãƒ©ãƒ³ãƒã§ä½¿ç”¨ã•ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã€‚
"""

import json
import re
import shutil
import sys
from pathlib import Path
from typing import Any


class BaselineUpdater:
    """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’æ›´æ–°."""

    def __init__(self):
        self.benchmark_results_dir = Path("benchmark_results")
        self.performance_dir = Path("tests/performance")

    def load_latest_results(self) -> dict[str, Any]:
        """æœ€æ–°ã®çµæœã‚’èª­ã¿è¾¼ã‚€."""
        latest_path = self.benchmark_results_dir / "latest.json"

        if not latest_path.exists():
            print(f"âŒ æœ€æ–°ã®çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {latest_path}")
            print("  å…ˆã«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„: pytest tests/performance/ -m benchmark")
            sys.exit(1)

        with open(latest_path) as f:
            return json.load(f)

    def validate_results(self, results: dict[str, Any]) -> bool:
        """çµæœã®å¦¥å½“æ€§ã‚’æ¤œè¨¼."""
        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒã‚§ãƒƒã‚¯
        required_fields = ["benchmarks", "environment", "timestamp"]
        for field in required_fields:
            if field not in results:
                print(f"âŒ å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒä¸è¶³: {field}")
                return False

        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®æ¤œè¨¼
        benchmarks = results.get("benchmarks", [])
        if len(benchmarks) < 3:  # æœ€ä½3ã¤ã®ãƒ†ã‚¹ãƒˆï¼ˆå˜ä¸€è¨ˆç®—ï¼‰
            print("âŒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãŒä¸è¶³")
            return False

        # å¿…é ˆãƒ†ã‚¹ãƒˆã®å­˜åœ¨ç¢ºèª
        test_names = [b["name"] for b in benchmarks]
        required_tests = [
            "test_quantforge_single",
            "test_pure_python_single",
            "test_numpy_scipy_single"
        ]

        for test in required_tests:
            if not any(test in name for name in test_names):
                print(f"âŒ å¿…é ˆãƒ†ã‚¹ãƒˆãŒä¸è¶³: {test}")
                return False

        return True

    def extract_benchmark_times(self, benchmarks: list[dict]) -> dict[str, dict[str, float]]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‹ã‚‰å®Ÿè¡Œæ™‚é–“ã‚’æŠ½å‡º."""
        results: dict[str, dict[str, float]] = {}

        for bench in benchmarks:
            name = bench["name"]
            mean_time = bench["stats"].get("mean", 0)

            # ãƒ†ã‚¹ãƒˆåã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ã‚«ãƒ†ã‚´ãƒ©ã‚¤ã‚º
            if "test_quantforge_single" in name:
                results.setdefault("single", {})["quantforge"] = mean_time
            elif "test_pure_python_single" in name:
                results.setdefault("single", {})["pure_python"] = mean_time
            elif "test_numpy_scipy_single" in name:
                results.setdefault("single", {})["numpy_scipy"] = mean_time
            elif "test_quantforge_batch" in name:
                size = self._extract_size(name)
                key = f"batch_{size}"
                results.setdefault(key, {})["quantforge"] = mean_time
            elif "test_pure_python_batch" in name:
                size = self._extract_size(name)
                key = f"batch_{size}"
                results.setdefault(key, {})["pure_python"] = mean_time
            elif "test_numpy_scipy_batch" in name:
                size = self._extract_size(name)
                key = f"batch_{size}"
                results.setdefault(key, {})["numpy_scipy"] = mean_time

        return results

    def _extract_size(self, name: str) -> int:
        """ãƒ†ã‚¹ãƒˆåã‹ã‚‰ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æŠ½å‡º."""
        match = re.search(r"\[(\d+)\]", name)
        if match:
            return int(match.group(1))
        return 0

    def format_time(self, time_seconds: float) -> str:
        """æ™‚é–“ã‚’é©åˆ‡ãªå˜ä½ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ."""
        if time_seconds < 1e-6:
            return f"{time_seconds * 1e9:.2f} ns"
        elif time_seconds < 1e-3:
            return f"{time_seconds * 1e6:.2f} Î¼s"
        elif time_seconds < 1:
            return f"{time_seconds * 1e3:.2f} ms"
        else:
            return f"{time_seconds:.2f} s"

    def update_baseline(self, output_path: Path = None) -> None:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’æ›´æ–°.

        Args:
            output_path: å‡ºåŠ›å…ˆãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: tests/performance/baseline.jsonï¼‰
        """
        if output_path is None:
            output_path = self.performance_dir / "baseline.json"

        # æœ€æ–°çµæœã‚’èª­ã¿è¾¼ã¿
        latest = self.load_latest_results()

        # æ¤œè¨¼
        if not self.validate_results(latest):
            print("âŒ çµæœã®æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ")
            sys.exit(1)

        # æ—¢å­˜ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
        if output_path.exists():
            backup_path = output_path.with_suffix(".json.bak")
            shutil.copy2(output_path, backup_path)
            print(f"ğŸ“ æ—¢å­˜ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_path}")

        # æ–°ã—ã„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¿å­˜ï¼ˆlatest.jsonã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼‰
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w") as f:
            json.dump(latest, f, indent=2)

        print(f"âœ… ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’æ›´æ–°ã—ã¾ã—ãŸ: {output_path}")

        # ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
        self.print_summary(latest)

    def print_summary(self, results: dict[str, Any]) -> None:
        """çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º."""
        print("\n" + "=" * 60)
        print("ğŸ“Š ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚µãƒãƒªãƒ¼")
        print("=" * 60)

        # ç’°å¢ƒæƒ…å ±
        env = results["environment"]
        print("\nç’°å¢ƒ:")
        print(f"  - ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ : {env.get('platform', 'N/A')}")
        print(f"  - Python: {env.get('python_version', 'N/A')}")
        print(f"  - CPU: {env.get('cpu_count', 'N/A')}ã‚³ã‚¢")

        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ™‚é–“ã‚’æŠ½å‡º
        times = self.extract_benchmark_times(results.get("benchmarks", []))

        # å˜ä¸€è¨ˆç®—
        if "single" in times:
            single = times["single"]
            print("\nå˜ä¸€è¨ˆç®—:")
            if "quantforge" in single:
                print(f"  - QuantForge: {self.format_time(single['quantforge'])}")
            if "pure_python" in single:
                print(f"  - Pure Python: {self.format_time(single['pure_python'])}")
            if "numpy_scipy" in single:
                print(f"  - NumPy+SciPy: {self.format_time(single['numpy_scipy'])}")

        # ãƒãƒƒãƒå‡¦ç†ï¼ˆä¸»è¦ãªã‚µã‚¤ã‚ºã®ã¿ï¼‰
        batch_sizes = [100, 1000, 10000, 100000]
        has_batch = False
        for size in batch_sizes:
            key = f"batch_{size}"
            if key in times and "quantforge" in times[key]:
                if not has_batch:
                    print("\nãƒãƒƒãƒå‡¦ç†ï¼ˆQuantForgeï¼‰:")
                    has_batch = True
                qf_time = times[key]["quantforge"]
                print(f"  - {size:7,}ä»¶: {self.format_time(qf_time)}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†."""
    import argparse

    parser = argparse.ArgumentParser(description="ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ›´æ–°")
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("tests/performance/baseline.json"),
        help="ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®å‡ºåŠ›å…ˆ"
    )
    parser.add_argument(
        "--check",
        action="store_true",
        help="æ›´æ–°ã›ãšã«ç¾åœ¨ã®çµæœã‚’ç¢ºèªã™ã‚‹ã®ã¿"
    )

    args = parser.parse_args()

    updater = BaselineUpdater()

    if args.check:
        # ç¢ºèªã®ã¿
        latest = updater.load_latest_results()
        if updater.validate_results(latest):
            print("âœ… çµæœã¯æœ‰åŠ¹ã§ã™")
            updater.print_summary(latest)
        else:
            print("âŒ çµæœãŒç„¡åŠ¹ã§ã™")
            sys.exit(1)
    else:
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ›´æ–°
        updater.update_baseline(args.output)


if __name__ == "__main__":
    main()
