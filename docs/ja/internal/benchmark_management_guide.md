# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç®¡ç†ã‚¬ã‚¤ãƒ‰

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€QuantForgeã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã¨`docs/performance/benchmarks.md`ã®æ›´æ–°æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿å„ªå…ˆã®åŸå‰‡
ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã¯**æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿**ã¨ã—ã¦ç®¡ç†ã•ã‚Œã€Markdownãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã™ã€‚

### ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæŒ‡å‘è¨­è¨ˆã¸ã®ç§»è¡Œ
2025å¹´8æœˆã®ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚Šã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚³ãƒ¼ãƒ‰ã¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæŒ‡å‘è¨­è¨ˆã«ç§»è¡Œã—ã¾ã—ãŸï¼š

- **67%ã®ã‚³ãƒ¼ãƒ‰å‰Šæ¸›**: å…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¸ã®çµ±åˆã«ã‚ˆã‚‹é‡è¤‡æ’é™¤
- **ä¿å®ˆæ€§ã®å‘ä¸Š**: åŸºåº•ã‚¯ãƒ©ã‚¹ã«ã‚ˆã‚‹çµ±ä¸€ã•ã‚ŒãŸã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
- **æ‹¡å¼µæ€§ã®å‘ä¸Š**: æ–°ã—ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®è¿½åŠ ãŒç°¡å˜ã«

```
benchmarks/                      # Pythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¨ã—ã¦æ§‹æˆ
â”œâ”€â”€ __init__.py                  # ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸åˆæœŸåŒ–
â”œâ”€â”€ __main__.py                  # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
â”œâ”€â”€ common/                      # å…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆNEWï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                  # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åŸºåº•ã‚¯ãƒ©ã‚¹
â”‚   â”‚   â”œâ”€â”€ TimingResult        # æ¸¬å®šçµæœã‚’æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹
â”‚   â”‚   â””â”€â”€ BenchmarkBase       # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã®åŸºåº•ã‚¯ãƒ©ã‚¹
â”‚   â”œâ”€â”€ formatters.py            # çµæœãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼
â”‚   â”‚   â”œâ”€â”€ format_time()       # æ™‚é–“ã‚’é©åˆ‡ãªå˜ä½ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
â”‚   â”‚   â”œâ”€â”€ format_throughput() # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
â”‚   â”‚   â””â”€â”€ BenchmarkFormatter  # Markdown/CSVå½¢å¼ã¸ã®å¤‰æ›
â”‚   â”œâ”€â”€ io.py                    # ãƒ•ã‚¡ã‚¤ãƒ«I/Oç®¡ç†
â”‚   â”‚   â””â”€â”€ BenchmarkIO         # çµæœã®ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ãƒ»æ¯”è¼ƒ
â”‚   â””â”€â”€ metrics.py               # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
â”‚       â”œâ”€â”€ calculate_speedup()      # é«˜é€ŸåŒ–ç‡ã®è¨ˆç®—
â”‚       â””â”€â”€ calculate_throughput()   # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®è¨ˆç®—
â”œâ”€â”€ baseline/                    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å®Ÿè£…
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ python_baseline.py      # Pure Pythonå®Ÿè£…
â”‚   â”œâ”€â”€ iv_baseline.py           # SciPyå®Ÿè£…
â”‚   â””â”€â”€ iv_vectorized.py         # ãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Ÿè£…
â”œâ”€â”€ runners/                     # å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ comparison.py            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œï¼ˆæ—§å®Ÿè£…ï¼‰
â”‚   â”œâ”€â”€ comparison_refactored.py # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œï¼ˆæ–°å®Ÿè£…ï¼‰
â”‚   â”œâ”€â”€ practical.py             # å®Ÿè·µã‚·ãƒŠãƒªã‚ª
â”‚   â””â”€â”€ arraylike.py             # ArrayLikeãƒ†ã‚¹ãƒˆ
â”œâ”€â”€ analysis/                    # åˆ†æãƒ„ãƒ¼ãƒ«
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ save.py                  # ãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒ»ç®¡ç†
â”‚   â”œâ”€â”€ analyze.py               # åˆ†æãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º
â”‚   â””â”€â”€ format.py                # Markdownç”Ÿæˆ
â””â”€â”€ results/                     # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆçœŸå®Ÿã®æºï¼‰
    â”œâ”€â”€ history.jsonl            # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ï¼ˆè¿½è¨˜å‹ï¼‰
    â”œâ”€â”€ latest.json              # æœ€æ–°çµæœ
    â””â”€â”€ history.csv              # åˆ†æç”¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

docs/performance/
â””â”€â”€ benchmarks.md                # è‡ªå‹•ç”Ÿæˆã•ã‚Œã‚‹è¡¨ç¤ºç”¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
```

## ğŸ”„ æ›´æ–°ãƒ•ãƒ­ãƒ¼

### 1. ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã¨è¨˜éŒ²

```{code-block} bash
:name: benchmark-management-guide-code-section
:caption: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œï¼ˆè‡ªå‹•çš„ã«å±¥æ­´ã«è¿½åŠ ï¼‰

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦å®Ÿè¡Œï¼ˆã©ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã§ã‚‚å¯èƒ½ï¼‰
python -m benchmarks                        # ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼è¡¨ç¤º
python -m benchmarks.runners.comparison     # æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
python -m benchmarks.runners.practical      # å®Ÿè·µã‚·ãƒŠãƒªã‚ªå®Ÿè¡Œ

# uvã§ã®å®Ÿè¡Œ
uv run python -m benchmarks.runners.comparison
```

å®Ÿè¡Œã«ã‚ˆã‚Šä»¥ä¸‹ãŒè‡ªå‹•çš„ã«è¡Œã‚ã‚Œã¾ã™ï¼š
1. `results/history.jsonl`ã«æ–°ã—ã„è¡Œã‚’è¿½åŠ 
2. `results/latest.json`ã‚’æ›´æ–°
3. `benchmark_results.json`ã‚’ç”Ÿæˆï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰

### 2. docs/performance/benchmarks.md ã®æ›´æ–°

#### æ–¹æ³•A: æœ€æ–°çµæœã®ã¿åæ˜ ï¼ˆæ¨å¥¨ï¼‰

```bash
# 1. æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®è¦ç´„ã‚’ç¢ºèªï¼ˆã©ã“ã‹ã‚‰ã§ã‚‚å®Ÿè¡Œå¯èƒ½ï¼‰
python -m benchmarks.analysis.analyze

# 2. benchmarks.mdã®è©²å½“ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ‰‹å‹•æ›´æ–°
# ä»¥ä¸‹ã®å€¤ã‚’æ›´æ–°ï¼š
# - æœ€æ–°æ¸¬å®šçµæœã®æ—¥ä»˜
# - å˜ä¸€è¨ˆç®—ã®å®Ÿæ¸¬å€¤
# - ãƒãƒƒãƒå‡¦ç†ã®å®Ÿæ¸¬å€¤
# - ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ
```

#### æ–¹æ³•B: å®Œå…¨ãªè‡ªå‹•ç”Ÿæˆ

```bash
# Markdownã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆï¼ˆã©ã“ã‹ã‚‰ã§ã‚‚å®Ÿè¡Œå¯èƒ½ï¼‰
python -c "
from benchmarks.analysis.analyze import generate_summary_table
print(generate_summary_table())
" > summary.md

# å¿…è¦ãªéƒ¨åˆ†ã‚’benchmarks.mdã«ã‚³ãƒ”ãƒ¼
```

#### æ–¹æ³•C: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã®æ›´æ–°

```{code-block} python
:name: benchmark-management-guide-code-benchmarksupdatedocspy
:caption: benchmarks/update_docs.py ã¨ã—ã¦ä¿å­˜

# benchmarks/update_docs.py ã¨ã—ã¦ä¿å­˜
from pathlib import Path
import json
from datetime import datetime
from benchmarks.analysis import save

def update_benchmarks_doc():
    """docs/performance/benchmarks.mdã‚’æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã§æ›´æ–°."""
    
    # æœ€æ–°ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    results_path = Path(__file__).parent / "results" / "latest.json"
    with open(results_path) as f:
        data = json.load(f)
    
    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆèª­ã¿è¾¼ã¿
    doc_path = Path(__file__).parent.parent / "docs" / "performance" / "benchmarks.md"
    content = doc_path.read_text()
    
    # æ—¥ä»˜æ›´æ–°
    date_str = datetime.fromisoformat(data["timestamp"]).strftime("%Y-%m-%d")
    content = content.replace(
        "## æœ€æ–°æ¸¬å®šçµæœï¼ˆ", 
        f"## æœ€æ–°æ¸¬å®šçµæœï¼ˆ{date_str}"
    )
    
    # å®Ÿæ¸¬å€¤æ›´æ–°
    qf_us = data["single"]["quantforge"] * 1e6
    content = update_value(content, "QuantForge (Rust) |", f"{qf_us:.2f} Î¼s")
    
    # æ›¸ãæˆ»ã—
    doc_path.write_text(content)
    print(f"âœ… Updated {doc_path}")

if __name__ == "__main__":
    update_benchmarks_doc()
```

### 3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ã®ãƒã‚§ãƒƒã‚¯

```bash
cd benchmarks

# å›å¸°æ¤œå‡º
uv run python -c "
from analyze import analyze_performance_trends
trends = analyze_performance_trends()
if 'regressions' in trends and trends['regressions']:
    print('âš ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°æ¤œå‡º:')
    for reg in trends['regressions']:
        print(f'  - {reg}')
else:
    print('âœ… å›å¸°ãªã—')
"
```

## ğŸ“ benchmarks.md æ›´æ–°ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### å¿…é ˆæ›´æ–°é …ç›®
- [ ] æ¸¬å®šæ—¥ä»˜ï¼ˆ`## æœ€æ–°æ¸¬å®šçµæœï¼ˆYYYY-MM-DDï¼‰`ï¼‰
- [ ] ãƒ†ã‚¹ãƒˆç’°å¢ƒã®CPU/ãƒ¡ãƒ¢ãƒªæƒ…å ±
- [ ] å˜ä¸€è¨ˆç®—ã®å®Ÿæ¸¬å€¤ï¼ˆÎ¼så˜ä½ï¼‰
- [ ] ãƒãƒƒãƒå‡¦ç†ã®å®Ÿæ¸¬å€¤ï¼ˆmså˜ä½ï¼‰
- [ ] ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼ˆM ops/secï¼‰

### ã‚ªãƒ—ã‚·ãƒ§ãƒ³æ›´æ–°é …ç›®
- [ ] FFIã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã®åˆ†æ
- [ ] ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç‰¹æ€§ã®ã‚°ãƒ©ãƒ•
- [ ] ç’°å¢ƒåˆ¥ã®æ¯”è¼ƒï¼ˆç•°ãªã‚‹CPUã§ã®æ¸¬å®šæ™‚ï¼‰

## ğŸ“¦ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‹ã‚‰ã®ä½¿ç”¨æ–¹æ³•

### ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¨ã—ã¦ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

```{code-block} python
:name: benchmark-management-guide-code-import
:caption: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ä½¿ç”¨ä¾‹

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å®Ÿè£…ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from benchmarks.baseline.python_baseline import (
    black_scholes_pure_python,
    black_scholes_numpy_batch
)
from benchmarks.baseline.iv_baseline import (
    black_scholes_price_scipy,
    implied_volatility_scipy
)

# åˆ†æãƒ„ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from benchmarks.analysis.analyze import (
    detect_performance_trends,
    generate_summary_table
)
from benchmarks.analysis.save import save_benchmark_result

# å®Ÿè¡Œä¾‹
result = black_scholes_pure_python(s=100, k=105, t=1.0, r=0.05, sigma=0.2)
print(f"Pure Python Result: {result:.4f}")

# åˆ†æã®å®Ÿè¡Œ
trends = detect_performance_trends()
summary = generate_summary_table()
```

### ã‚«ã‚¹ã‚¿ãƒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®ä½œæˆ

#### æ—§å®Ÿè£…ï¼ˆæ‰‹å‹•æ¸¬å®šï¼‰

```{code-block} python
:name: benchmark-management-guide-code-custom-old
:caption: ã‚«ã‚¹ã‚¿ãƒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®æ—§å®Ÿè£…

import time
from benchmarks.analysis.save import save_benchmark_result

def custom_benchmark():
    """ç‹¬è‡ªã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè£…ï¼ˆæ—§ã‚¹ã‚¿ã‚¤ãƒ«ï¼‰."""
    results = {}
    
    # æ¸¬å®šå¯¾è±¡ã®å®Ÿè£…ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    from benchmarks.baseline.python_baseline import black_scholes_pure_python
    from quantforge import models
    
    # æ¸¬å®š
    start = time.perf_counter()
    for _ in range(10000):
        black_scholes_pure_python(100, 105, 1.0, 0.05, 0.2)
    pure_python_time = time.perf_counter() - start
    
    start = time.perf_counter()
    for _ in range(10000):
        models.call_price(100, 105, 1.0, 0.05, 0.2)
    quantforge_time = time.perf_counter() - start
    
    # çµæœã‚’ä¿å­˜
    results = {
        "pure_python": pure_python_time,
        "quantforge": quantforge_time,
        "speedup": pure_python_time / quantforge_time
    }
    
    save_benchmark_result(results)
    return results
```

#### æ–°å®Ÿè£…ï¼ˆå…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨ï¼‰

```{code-block} python
:name: benchmark-management-guide-code-custom-new
:caption: ã‚«ã‚¹ã‚¿ãƒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®æ–°å®Ÿè£…ï¼ˆæ¨å¥¨ï¼‰

from benchmarks.common import BenchmarkBase, BenchmarkFormatter, BenchmarkIO
from benchmarks.common.metrics import calculate_speedup
from typing import Any

class CustomBenchmark(BenchmarkBase):
    """ç‹¬è‡ªã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè£…ä¾‹."""
    
    def __init__(self):
        super().__init__(warmup_runs=100, measure_runs=1000)
        
    def run(self) -> dict[str, Any]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ."""
        self.start_benchmark()
        
        results = {
            "system_info": self.get_system_info(),
            "comparison": self.benchmark_comparison(),
        }
        
        # çµæœã‚’è‡ªå‹•ä¿å­˜
        io = BenchmarkIO()
        io.save_result(results)
        
        # Markdownå½¢å¼ã§å‡ºåŠ›
        formatter = BenchmarkFormatter("Custom Benchmark Results")
        print(formatter.format_markdown(results))
        
        return results
    
    def benchmark_comparison(self) -> dict[str, Any]:
        """ã‚«ã‚¹ã‚¿ãƒ æ¸¬å®šã®å®Ÿè£…."""
        from benchmarks.baseline.python_baseline import black_scholes_pure_python
        from quantforge import models
        
        # time_function ãƒ¡ã‚½ãƒƒãƒ‰ã§è‡ªå‹•æ¸¬å®š
        pure_python_timing = self.time_function(
            lambda: black_scholes_pure_python(100, 105, 1.0, 0.05, 0.2)
        )
        
        quantforge_timing = self.time_function(
            lambda: models.call_price(100, 105, 1.0, 0.05, 0.2)
        )
        
        return {
            "pure_python": {
                "median": pure_python_timing.median,
                "mean": pure_python_timing.mean,
                "std": pure_python_timing.std,
            },
            "quantforge": {
                "median": quantforge_timing.median,
                "mean": quantforge_timing.mean,
                "std": quantforge_timing.std,
            },
            "speedup": calculate_speedup(
                pure_python_timing.median, 
                quantforge_timing.median
            ),
        }

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    benchmark = CustomBenchmark()
    results = benchmark.run()
```

### å®Ÿè£…ã®æ¯”è¼ƒ

| é …ç›® | æ—§å®Ÿè£… | æ–°å®Ÿè£…ï¼ˆcommonä½¿ç”¨ï¼‰ |
|------|---------|-----------------------|
| ã‚³ãƒ¼ãƒ‰è¡Œæ•° | å„ãƒ•ã‚¡ã‚¤ãƒ«200-300è¡Œ | åŸºåº•ã‚¯ãƒ©ã‚¹ç¶™æ‰¿ã§100è¡Œç¨‹åº¦ |
| æ™‚é–“æ¸¬å®š | æ‰‹å‹•å®Ÿè£… | TimingResultè‡ªå‹•ç®¡ç† |
| ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ | å„ãƒ•ã‚¡ã‚¤ãƒ«ã§é‡è¤‡å®Ÿè£… | BenchmarkFormatterçµ±ä¸€ |
| ãƒ‡ãƒ¼ã‚¿ä¿å­˜ | æ‰‹å‹•JSONæ“ä½œ | BenchmarkIOè‡ªå‹•ç®¡ç† |
| ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ— | æ‰‹å‹•ã¾ãŸã¯ãªã— | è‡ªå‹•å®Ÿæ–½ |
| çµ±è¨ˆæƒ…å ± | æœ€å°é™ | å¹³å‡ã€ä¸­å¤®å€¤ã€æ¨™æº–åå·®ç­‰ |

## ğŸ” ãƒ‡ãƒ¼ã‚¿åˆ†æã‚³ãƒãƒ³ãƒ‰

### å±¥æ­´ã®çµ±è¨ˆæƒ…å ±

```{code-block} bash
:name: benchmark-management-guide-code-section
:caption: å…¨å±¥æ­´ã®è¦ç´„çµ±è¨ˆ

# å…¨å±¥æ­´ã®è¦ç´„çµ±è¨ˆï¼ˆã©ã“ã‹ã‚‰ã§ã‚‚å®Ÿè¡Œå¯èƒ½ï¼‰
python -c "
from benchmarks.analysis.analyze import load_history
import json

history = load_history()

print(f'æ¸¬å®šå›æ•°: {len(history)}')
if history:
    print(f'æœŸé–“: {history[0]["timestamp"]} ~ {history[-1]["timestamp"]}')
    
    # QuantForgeå˜ä¸€è¨ˆç®—ã®çµ±è¨ˆ
    qf_times = [h['single']['quantforge'] * 1e6 for h in history if 'single' in h]
    if qf_times:
        print(f'QuantForgeå˜ä¸€è¨ˆç®—:')
        print(f'  æœ€å°: {min(qf_times):.2f} Î¼s')
        print(f'  æœ€å¤§: {max(qf_times):.2f} Î¼s')
        print(f'  å¹³å‡: {sum(qf_times)/len(qf_times):.2f} Î¼s')
"
```

### CSVå‡ºåŠ›ã¨å¤–éƒ¨ãƒ„ãƒ¼ãƒ«é€£æº

```{code-block} bash
:name: benchmark-management-guide-code-csv
:caption: CSVç”Ÿæˆ

# CSVç”Ÿæˆ
cd benchmarks
uv run python save_results.py

# pandasã§åˆ†æï¼ˆè¦pandasï¼‰
python -c "
import pandas as pd
df = pd.read_csv('results/history.csv')
print(df.describe())
print('\nç›¸é–¢è¡Œåˆ—:')
print(df[['single_quantforge_us', 'batch_1m_quantforge_ms']].corr())
"
```

## âš ï¸ æ³¨æ„äº‹é …

### ã‚„ã£ã¦ã¯ã„ã‘ãªã„ã“ã¨

1. **âŒ Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ‰‹å‹•ã§å¤§å¹…ç·¨é›†**
   - æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ãŒçœŸå®Ÿã®æº
   - Markdownã¯è¡¨ç¤ºç”¨

2. **âŒ æ—¥ä»˜ä»˜ãMarkdownãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ**
   - `benchmarks_YYYYMMDD.md`ã¯ä¸è¦
   - å±¥æ­´ã¯`history.jsonl`ã§ç®¡ç†

3. **âŒ æ¸¬å®šå€¤ã®æ‰‹å‹•å…¥åŠ›**
   - å¿…ãšå®Ÿéš›ã®æ¸¬å®šçµæœã‚’ä½¿ç”¨
   - ç†è«–å€¤ã‚„æ¨å®šå€¤ã¯è¨˜è¼‰ã—ãªã„

### æ¨å¥¨äº‹é …

1. **âœ… å®šæœŸçš„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ**
   - é€±1å›ç¨‹åº¦ã®å®šæœŸå®Ÿè¡Œ
   - å¤§ããªå¤‰æ›´å¾Œã¯å¿…ãšå®Ÿè¡Œ

2. **âœ… ç’°å¢ƒæƒ…å ±ã®è¨˜éŒ²**
   - CPUã€ãƒ¡ãƒ¢ãƒªã€OSæƒ…å ±ã¯è‡ªå‹•è¨˜éŒ²ã•ã‚Œã‚‹
   - ç‰¹æ®Šãªç’°å¢ƒè¨­å®šãŒã‚ã‚Œã°è¿½è¨˜

3. **âœ… å›å¸°æ¤œå‡ºã®è‡ªå‹•åŒ–**
   - CI/CDã¸ã®çµ±åˆã‚’æ¤œè¨
   - 5%ä»¥ä¸Šã®æ€§èƒ½åŠ£åŒ–ã§è­¦å‘Š

## ğŸš€ å°†æ¥ã®æ”¹å–„æ¡ˆ

### è‡ªå‹•åŒ–ã®å¼·åŒ–
- GitHub Actionsã§ã®å®šæœŸå®Ÿè¡Œ
- PRã”ã¨ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒ
- è‡ªå‹•çš„ãªbenchmarks.mdæ›´æ–°

### å¯è¦–åŒ–ã®æ”¹å–„
- Web UIã§ã®ã‚°ãƒ©ãƒ•è¡¨ç¤º
- ç’°å¢ƒåˆ¥ã®æ¯”è¼ƒãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

### ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã®æ‹¡å¼µ
- PostgreSQL/SQLiteã¸ã®ç§»è¡Œ
- ã‚¿ã‚°ä»˜ã‘æ©Ÿèƒ½ï¼ˆãƒªãƒªãƒ¼ã‚¹ç‰ˆã€å®Ÿé¨“ç‰ˆï¼‰
- A/Bãƒ†ã‚¹ãƒˆæ©Ÿèƒ½

## ğŸ† æœ€è¿‘ã®æ”¹å–„æˆæœ

### 2025å¹´8æœˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°
å…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å°å…¥ã«ã‚ˆã‚Šå¤§å¹…ãªæ”¹å–„ã‚’é”æˆï¼š

- **ã‚³ãƒ¼ãƒ‰å‰Šæ¸›**: 2,080è¡Œ â†’ 680è¡Œï¼ˆ67%å‰Šæ¸›ï¼‰
- **é‡è¤‡æ’é™¤**: åŒã˜æ¸¬å®šãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¸€å…ƒåŒ–
- **ä¿å®ˆæ€§**: ä¿®æ­£ç®‡æ‰€ã‚’1ãƒµæ‰€ã«é›†ç´„
- **æ‹¡å¼µæ€§**: æ–°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¿½åŠ ãŒç°¡å˜ã«

## é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«

- [benchmarks.md](../performance/benchmarks.md) - å…¬é–‹ç”¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- [benchmarks/README.md](../../benchmarks/README.md) - ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ„ãƒ¼ãƒ«ã®èª¬æ˜
- [naming_conventions.md](naming_conventions.md) - å‘½åè¦å‰‡ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åï¼‰